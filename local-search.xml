<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Light-weight</title>
    <link href="/2021/09/23/Light-weight/"/>
    <url>/2021/09/23/Light-weight/</url>
    
    <content type="html"><![CDATA[<h1 id="About-Light-Weight"><a href="#About-Light-Weight" class="headerlink" title="About Light-Weight"></a>About Light-Weight</h1><p>这篇主要来总结和了解一下关于轻量化网络设计的一些知识。</p><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><ul><li><p><strong>感受野（Receptive Field）</strong></p><p>指的是每一层输出的特征图（feature map）上每个像素点映射回输入图像上的区域大小。范围越大，就代表能够学习到更加全局、层次更高的特征，相反的是，范围越小说明能够学到局部的细节。所以网络越深，感受野越大。</p></li><li><p><strong>深度（Depth）</strong></p><p>一般来说，深度决定了网络的表达能力，早期的backbone通过堆叠卷积层的方法，后期通过module堆叠的方法。通常越深的网络表达能力越强，但也可能有副作用，所以需要调参！</p></li><li><p><strong>宽度（Width）</strong></p><p>宽度决定了某一层学到的信息量，但网络的宽度时指的是最大的通道数，有卷积核数量最多的层决定。通常在最后一层feature map达到最大，因为分辨率越小，包含的信息越高级，通道越多效果越好。但是计算量会他别打，通常通道数按照8的倍数来决定。</p></li><li><p><strong>分辨率（Resolution）</strong></p><p>指的是输入模型的图像尺寸，通常情况会根据模型下采样次数$N$和最后一次下采样后feature map的分辨率$k\times k$来决定输入分辨率大小，即$r = k\times2^{n}$</p></li><li><p><strong>分组卷积（Group convolution）</strong></p><p>最早提出在AlexNet中，那时GPU的性能还不支持全部卷积同时操作，所以作者提出把feature map分给多个GPU进行处理，最后再融合。如下图：</p><p><img src="/2021/09/23/Light-weight/img1.png" alt="img1"></p><p>图中输入的数据再深度上被分成的g组，具体的数量由$(C1/g)$决定，相应的每一组的卷积核的深度也变成了$(C1/g)$，每一组的个数变成了$(C2/g)$。最后通过concat进行组合。例如输入为256，输出为256，$kernel size = 3\times3$，普通卷积的参数量为$256\times3\times3\times256$，若g=8，则参数量变为$8\times32\times3\times3\times32$，减少了8倍。</p></li></ul><ul><li><p><strong>空洞卷积（Dilated Convoluton）</strong></p><p>空洞卷积针对下采样降低图片分辨率而提出，通过间隔取值来扩大感受野，让原本$3\times3$的卷积核，在参数量和计算量相同的情况下拥有更大的感受野。通过设置你的扩张率系数（决定了间隔大小），标准卷积的扩张率为1，下图为扩张率为2的空洞卷积</p><p><img src="/2021/09/23/Light-weight/img2.png" alt="img2"></p><p>相当于用一个$3\times3$的卷积大小来充当一个$5\times5$的卷积核大小，类似于间隔补0。</p><p>但它的缺点也很明显，并不会采集到所有像素的信息，对一些小目标来说，效果也不太好。</p></li><li><p><strong>转置卷积（Transposed Convolutions）</strong></p><p>与空洞卷积相反，为上采样而提出，对特征图进行间隔补0，卷积核大小不变，来得到更大尺寸的特征图。</p></li><li><p><strong>可变形卷积（deformable convolution）</strong></p></li></ul><ul><li><p><strong>分组点卷积（Poinstwise group convolution）</strong></p><p>点卷积采用了密集的未分组的$1\times1$点卷积，大量的$1\times1$的卷积造成了极大的算力负担，所以能否给点卷积也分组呢？</p></li><li></li></ul><h1 id="经典模型"><a href="#经典模型" class="headerlink" title="经典模型"></a>经典模型</h1><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>原论文如下说到</p><blockquote><p>So what have we gained by using, for instance, a stack of three $3\times3$ conv. layers instead of a single $7\times7$ layer? First, we incorporate three non-linear rectifification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer $3\times3$ convolution stack has C channels, the stack is parametrised by $3(3^{2}C^{2})$ = 27$C^{2}$ weights; at the same time, a single $7\times7$ conv. layer would require $7^{2}C^{2}$ = 49$C^{2}$ parameters</p></blockquote><p>从轻量化角度来说，利用$3\times3$的卷积核来代替其他的卷积核，在拥有相同感受野的情况下，能够做到参数量小的优点。</p><h2 id="MobileNet系列"><a href="#MobileNet系列" class="headerlink" title="MobileNet系列"></a>MobileNet系列</h2><ul><li>MobileNet V1 提出了深度可分离卷积</li></ul><p>​    </p><h2 id="ShuffleNet系列"><a href="#ShuffleNet系列" class="headerlink" title="ShuffleNet系列"></a>ShuffleNet系列</h2><ul><li><p>ShuffleNet V1，引入了分组点卷积（pointwise group convolution）和通道重排（channel shuffle）两个新操作，保持精度同时减少算力消耗。但分组点卷积的输出仅来自于一小部分的输入通道，组织了信息的流通和特征表示，所以非常自然而然地想到如果从不同组里获取输入数据，那么输入和输出通道是不是就相关了呢。</p><p><strong>通道重排（Channel shuffle）：</strong>首先假定某卷积层分为$g$组，输出特征通道数为$g\times g$，再使用$reshape$操作将$gn$个通道数变为维度为$(g,n)$，之后使用$transpose$转置，最后使用$flatten$展开做为下一组卷积的输入。如下图，$g=3,n=4$</p><p><img src="/2021/09/23/Light-weight/shuffle.png" alt="shuffle"></p><p><strong>ShuffleNet Unit：</strong>第一种是将ResNet中的bottleneck，将标准卷积替换为深度卷积，第二种是最常用的，也就是将密集的点卷积替换成分组点卷积，第三种就是针对降采样情况，将元素的add变为通道的叠加。</p><p><img src="/2021/09/23/Light-weight/shuffle2.png" alt="shuffle2"></p><p><strong>FLOPs：</strong>输入特征为$h\times w\times c$，Bottleneck通道数为$m$，分组数为$g$，$F = hw\frac{c}{g}\frac{m}{g}g + 3\times3hwm + hw\frac{c}{g}\frac{m}{g}g = hw(\frac{2cm}{g} + 9m)$对比于ResNet的$F = hw(2cm + 9m^{2})$减少了很多计算量。</p><p><strong>网络结构：</strong></p></li><li><p>shuffleNet V2，总共有四个新发现：</p><ol><li>当输入输出通道数相同时，运算速度最快，<img src="/2021/09/23/Light-weight/shufflev2.png" alt="shufflev2"></li><li>过度使用组卷积增加额外算力消耗，<img src="/2021/09/23/Light-weight/shufflev2_1.png" alt="shufflev2_1"></li><li>Element-wise operation消耗较多的时间，<img src="/2021/09/23/Light-weight/shufflev2_2.png" alt="shufflev2_2"></li><li>过度使用组卷积额外增加算力消耗，<img src="/2021/09/23/Light-weight/shufflev2_3.png" alt="shufflev2_3"></li></ol><p>针对这些问题，作者的创新与改进：</p><ol><li>channel split，将输入特征通道一部分进入bottlenck，另一部分不操作（输入输出通道数相同）</li><li>替换分组点卷积为标准点卷积（防止过度点卷积）</li><li>channel shuffle放到维度叠加后（防止碎片化）</li><li>将element-wise add替换为concat（减少时间消耗）</li></ol><p><img src="/2021/09/23/Light-weight/shufflev2_4.png" alt="shufflev2_4"></p><p><strong>网络结构：</strong></p><p><img src="/2021/09/23/Light-weight/shufflev2_5.png" alt="shufflev2_5"></p><p>在全连接层前加一层标准卷积Conv5，类似MovileNet思想，通过超参数来所见通道</p></li></ul><h2 id="GhostNet"><a href="#GhostNet" class="headerlink" title="GhostNet"></a>GhostNet</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>FCOS</title>
    <link href="/2021/09/16/FCOS/"/>
    <url>/2021/09/16/FCOS/</url>
    
    <content type="html"><![CDATA[<h1 id="FCOS-Fully-Convolutional-One-Stage-Object-Detection"><a href="#FCOS-Fully-Convolutional-One-Stage-Object-Detection" class="headerlink" title="FCOS: Fully Convolutional One-Stage Object Detection"></a>FCOS: Fully Convolutional One-Stage Object Detection</h1><p>​    FCOS是一个基于FCN（全卷积网络）、一阶段（one stage）、anchor free、参考语义分割实现逐像素的单阶段目标检测。原论文<a href="https://arxiv.org/pdf/1904.01355v5.pdf">地址</a>.</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><blockquote><p>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks</p></blockquote><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><ul><li>FCOS方法借鉴了FCN的思想，在每个feature map上的特征点做回归操作，预测（l，r，t，d）四个值，代表了到GTBox的上、下、左、右的距离。</li><li>引入了FPN结构，利用不同的层来处理不同尺寸的目标框。</li><li>引入了Centerness Layer来增强中心点选取的准确性</li><li>分类损失focal loss；回归损失iou loss；centerness损失BCE</li></ul><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Anchor-based的缺点"><a href="#Anchor-based的缺点" class="headerlink" title="Anchor-based的缺点"></a>Anchor-based的缺点</h2><ul><li>anchor-base模型的检测性能一定程度上依赖于anchor的设计，anchor的基础尺寸、长宽比、以及每一个特征点对应的anchor数目等。比如Faster提出的基准anchor大小16，3种倍数[8, 16, 32] 以及三种比例，共9种anchor。</li><li>设定好anchor了只能说是匹配到大部分目标，对于那些形变较大的目标检测起来还是比较困难，同时这也一定程度上限制了模型的泛化能力。</li><li>为了取得较好的召回率，那就需要为每个特征点安排更密集的anchor，在前向推演以及NMS等操作时，显存以及CPU消耗很大。</li><li>在这些放置的更密集的anchor中，大多数anchor属于负样本，这样也造成了正负样本之间的不均衡。（Faster好像各选128 positive / negtive 作为训练anchor，不过肯定不是随机挑选的）</li></ul><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>下图是FCOS的模型结构：</p><p><img src="/2021/09/16/FCOS/img1.png" alt="img1"></p><p>整体还是非常传统的Bckbone+FPN+head结构</p><h2 id="论文中的一些定义"><a href="#论文中的一些定义" class="headerlink" title="论文中的一些定义"></a>论文中的一些定义</h2><ul><li><p>第i个GTbox的定义为：$B_{i}$,其中$(x_{0}^{(i)},y_{0}^{(i)})$，$(x_{1}^{(i)},y_{1}^{(i)})$表示的是当前Bbox左上角和右下角的坐标。</p></li><li><p>回归分支预测的为当前点$(X,Y)$到GTBox边界的距离$(l,r,t,d)$，真实标签定义为$(l^{*},r^{*},t^{*},d^{*})$。值如下计算：</p></li><li><p>映射回的点如果在一个GTBox内就归为正样本，类别为BBOX内的目标类别，其余归为负样本。</p></li><li><p>按照s(下采样总倍数)将特征点映射回原图，参照公式</p><script type="math/tex; mode=display">(x^{'},y^{'}) = (floor(\frac{s}{2}) + xs, floor(\frac{s}{2}) + ys)\tag1</script><p>特征点上的点为$(x,y)$，原图位置为$(x^{‘},y^{‘})$</p></li></ul><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p>当一个$(x^{‘’},y^{‘’})$落在多个GTBox中，该论文将该特征点称为模糊样本，论文中说道：</p><blockquote><p>If a location falls into multiple bounding boxes, it is considered as an ambiguous sample.We simply choose the bounding box with minimal area as its regression target.</p></blockquote><p>论文简单的选择面积更小的GTBox做为回归目标。</p><p>与anchor-based的模型不同，FCOS选择了设置阈值来限制每一层feature map的回归范围。具体操作如下</p><ol><li>首先计算出不同层级feature map上每个特征点对应的回归目标$(l^{*},r^{*},t^{*},d^{*})$</li><li>如果特征点满足$max((l^{*},r^{*},t^{*},d^{*})&gt;m_{i})max((l^{*},r^{*},t^{*},d^{*})&lt;m_{i-1})$，将该样本认定为负样本。</li><li>m_{i}表示为第i层feature map回归的做大距离，本文设置了$(m_{2},m_{3},m_{4},m_{5},m_{6},m_{7} = 0,64,128,258,512,\infty)$，例如p3回归的范围为[0,64]，p4范围就为[64,128]</li></ol><p>如果回归在了两个GTBox里，就要分为两种情况</p><ol><li>假定一个格子表示16px，A距离紫色边界的最大距离为48px，距离绿色的最大距离为90px，根据p3的范围，对应的是紫色框，如果没有GTBox，就为负样本。</li><li>如果一个格子表示8px，那么A距离紫色边界最大距离为24px，距离绿色最大距离为40px，在都符合要求的情况下选择面积更小的作为回归目标</li><li><img src="/2021/09/16/FCOS/img2.png" alt="img2"></li></ol><h2 id="Center-ness-Layer"><a href="#Center-ness-Layer" class="headerlink" title="Center-ness Layer"></a>Center-ness Layer</h2><p>​    由于一个特征点映射到原图的时候可能位于GTBox的边缘或者距离中心较远的位置，因此在模型学习的时候可能会认为这个点不属于它本该的目标，导致了出现新的box。</p><p>​    为了解决这个问题，提出了中心度的思想，告诉哪些点是目标最可能的中心点（告诉某些点不是中心点）。</p><p>​    将目标中心点的值设定为1，距离越远该值就越趋近于0。真实值标签计算方法如下公式所示：</p><script type="math/tex; mode=display">centerness^{*} = \sqrt{\frac{min(l^{*},r^{*})}{max(l^{*}{r^{*}})}\times\frac{min(t^{*},b^{*})}{max(t^{*},b^{*})}}\tag2</script><p>​    这部分的损失使用<a href="https://blog.csdn.net/geter_CS/article/details/84747670?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163184001516780264071086%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163184001516780264071086&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-84747670.first_rank_v2_pc_rank_v29&amp;utm_term=bce%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1018.2226.3001.4187">BCE</a>损失。</p><p>​    模型的最终输出是一组置信度高的box和类别，然后使用NMS进行抑制筛选。模型的最终输出置信度=(类别的概率)*(对应的center-ness)。</p><p>​    center-ness会让模型更加关注到距离真实目标中心点近的预测box。</p><p>​    下图的实验证明了这样的相乘是有意义的，置信度越高预测出来的box和GTBox的IOU越高。</p><p>​    <img src="/2021/09/16/FCOS/img3.png" alt="img3"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul><li>分类损失，弃用了softmax，对head输出的分类的每一个通道分别使用sigmoid函数，最后使用Focal loss来计算。</li><li>回归损失：IOU Loss，对有意义的特征点进行回归，$c^{*}_{x,y}=0$ 就是不在一个GTBox的对应点，为负样本。如果在一个或者多个box，根据阈值重复筛选直到有意义即$c^{*}_{x,y}&gt;0$</li><li>center-ness损失：BCE损失，为什么没有在论文公式图里？？？？</li></ul><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><p>​    在引入FPN的同时还引入一些改变，FCOS在不同的特征层级之间共享head（共享的是结构/权重共享 ）。但是不同的特征层级需要回归不同的大小范围（例如，P3的大小范围是[0，64]，P4的大小范围是[64，128]，因此对于不同的特征层级使用相同的head是不合理的。因此，论文中不再使用标准的$exp(x)$，而是使用$exp( s_{i}* x)$，其中可训练标量  被用来自动调整不同层级特征的指数函数的基数，从而稍微提高了检测性能</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>可以引入一些提升性能的trick，比如GN，可变性卷积等</li><li>实现了proposal free 和 anchor free，避免了复杂的iou计算了匹配，（但是翻了一下别人的博客说用特征分布有差异的数据集训练普适性不是很好。</li><li>修改一下可以做实例分割、关键点检测等</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Introduction</title>
    <link href="/2021/09/12/Introduction/"/>
    <url>/2021/09/12/Introduction/</url>
    
    <content type="html"><![CDATA[<p>Welcome to my blog！欢迎来到我的博客，在浏览我的博客之前，希望你可以先看一下一些introduction以便你更好得了解我。</p><h1 id="关于我的博客"><a href="#关于我的博客" class="headerlink" title="关于我的博客"></a>关于我的博客</h1><p>这是Mr.ind1fferent的博客，通过github+hexo搭建完成，使用的是sakura主题（很多还没有配完，待完善，也不知道要不要完善：）</p><p>至于为什么要叫Mr.ind1fferent，是因为高中英语李老师和我聊天的时候根据我的名字给我取的，至今令我印象深刻。</p><h1 id="为什么要写博客"><a href="#为什么要写博客" class="headerlink" title="为什么要写博客"></a>为什么要写博客</h1><ul><li>做为计算机的学生，我觉得不仅仅是要把本科的学完，课上传授的知识不够你本科毕业去寻找工作，写博客可以把你在课余时间学习到的知识记录下来，提供你学习的动力</li><li>博客也是一种记录你生活的一个方式，慢慢得积累下去，你会怀念这段时光。</li><li>博客能够更好地帮助你理解你所学的知识，希望所有计算机地学子们能够不忘初心。</li></ul><h1 id="一些闲话"><a href="#一些闲话" class="headerlink" title="一些闲话"></a>一些闲话</h1><ul><li><p>博客的一些相关配置还没有完全弄好（会尽快！），最近应该会把时间放在写博客上面</p></li><li><p>由于能力有限，我会尽我所能把我阅读的论文正确地分享给大家，一些比较久远的论文可能就会大概地讲一下。</p></li><li><p>深度学习的代码大部分会基于pytorch，但不排除有些会用paddlepaddle和mindspore</p></li></ul><h1 id="关于深度学习"><a href="#关于深度学习" class="headerlink" title="关于深度学习"></a>关于深度学习</h1><p>很多刚接触这方面的小白，可能会觉得深度学习非常的高深莫测，实际上并不全是。</p><p>从2020年底学习深度学习（下文简称“DL”）到现在，有一些体会想分享给大家。</p><ol><li><p>打好基础</p><p>众所周知现在DL都会建立在python这门语法，学好基础的语法知识有助于理解</p></li><li><p>不要放弃</p><p>在刚开始入门时，你会接触到很多的数学公式，这可能对一些同学不是很友好，确实DL的理论学习刚开始是非常困难的。</p></li><li><p>提高英语水平</p><p>在学习DL的过程中，避免不了地会遇到许多专业名词和论文阅读，这需要你有一定的英语基础。</p></li><li><p>进实验室或者社团</p><p>一般的电脑并不支持DL的训练任务，这时候就需要你进入实验室或者社团，利用这里的硬件资源，这样你有新的idea的时候也比较容易去实现</p></li></ol><h1 id="关于比赛心得"><a href="#关于比赛心得" class="headerlink" title="关于比赛心得"></a>关于比赛心得</h1><ol><li>首先不要认为你的技术不够或者很多东西没有学而畏惧比赛，基本上所有人都是边比赛边学习。</li><li>不要报着功利的态度去参加比赛</li><li>尽量选择一些能够用在多项比赛的项目，一举多得</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>各类激活函数介绍</title>
    <link href="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/"/>
    <url>/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是激活函数？"><a href="#什么是激活函数？" class="headerlink" title="什么是激活函数？"></a>什么是激活函数？</h1><p>激活函数(Activation functions)对于神经网络模型去学习、理解复杂和非线性的函数具有十分重要的作用。将非线性特性引入到网络当中。</p><p>如下图的网络图</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img1.png" alt="img1"></p><p>那到底网络是通过什么操作来的到最后一个节点的输出呢？没错，就是<strong>激活函数。</strong></p><h1 id="为什么要使用激活函数？"><a href="#为什么要使用激活函数？" class="headerlink" title="为什么要使用激活函数？"></a>为什么要使用激活函数？</h1><ol><li>激活函数对模型学习和理解有着重要的作用</li><li>激活函数提供非线性的因素。如果不使用，只是输出了一个简单的线性函数，复杂度十分有限，从数据中学习复杂函数的映射能力有限，神经网络将无法学习和模拟其他复杂类型的数据，例如语音、视频、图像等等</li><li>激活函数能够把特征空间通过一定的线性映射转换到另一个空间，能够更好地被分类。</li></ol><h1 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h1><ul><li><h2 id="sigmoid激活函数"><a href="#sigmoid激活函数" class="headerlink" title="sigmoid激活函数"></a><strong>sigmoid激活函数</strong></h2><p>函数的定义方程如下图</p><script type="math/tex; mode=display">f(x) = \frac{1}{1+e^{-x}}</script><p>值域为<code>(0,1)</code>。函数图像如下</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img2.png" alt="img2"></p><p><strong>优点：</strong>它能够将输入的连续值变为0和1之间的输出，如果是非常大的负数，输出就是0；非常大的正数，输出就是1。</p><p><strong>缺点：</strong>1.容易在反向传播时导致<a href="https://blog.csdn.net/raojunyang/article/details/79962665?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-79962665.nonecase&amp;spm=1018.2226.3001.4187">梯度爆炸</a>和<a href="https://blog.csdn.net/raojunyang/article/details/79962665?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-79962665.nonecase&amp;spm=1018.2226.3001.4187">梯度消失</a>（直接上超链接了以后有空写）。函数的导数如下图：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img3.png" alt="img3"></p><p>​            2.不是以0为对称轴</p><p><strong>函数及其导数的代码参考实现如下：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">d_sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    y = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>    dy = y * (<span class="hljs-number">1</span> - y)<br>    <span class="hljs-keyword">return</span> dy<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    y = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>    <span class="hljs-keyword">return</span> y<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_sigmoid</span>():</span><br>    <span class="hljs-comment"># param:起点，终点，间距</span><br>    x = np.arange(-<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0.2</span>)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    plt.title(<span class="hljs-string">'sigmoid'</span>)  <span class="hljs-comment"># 第一幅图片标题</span><br>    y = sigmoid(x)<br>    plt.plot(x, y)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    y = d_sigmoid(x)<br>    plt.plot(x, y)<br>    plt.title(<span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">d_sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    y = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>    dy = y * (<span class="hljs-number">1</span> - y)<br>    <span class="hljs-keyword">return</span> dy<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    y = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>    <span class="hljs-keyword">return</span> y<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_sigmoid</span>():</span><br>    <span class="hljs-comment"># param:起点，终点，间距</span><br>    x = np.arange(-<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0.2</span>)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    plt.title(<span class="hljs-string">'sigmoid'</span>)  <span class="hljs-comment"># 第一幅图片标题</span><br>    y = sigmoid(x)<br>    plt.plot(x, y)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    y = d_sigmoid(x)<br>    plt.plot(x, y)<br>    plt.title(<span class="hljs-string">'sigmoid导数'</span>)<br>    plt.show()<br></code></pre></td></tr></tbody></table></figure></li></ul><ul><li><h2 id="tanh激活函数"><a href="#tanh激活函数" class="headerlink" title="tanh激活函数"></a><strong>tanh激活函数</strong></h2><p>函数的定义方程如下图：</p><script type="math/tex; mode=display">tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><p>值域为(-1,1)。函数图像如下：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img4.png" alt="img4"></p></li></ul><p>​        导数如下：</p><p>​        <img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img5.png" alt="img5"></p><p>​        该函数解决了Sigmoid函数不是zero-centered输出问题，然后梯度消失问题和幂运算的问题仍然存在。</p><p>​        <strong>优点：</strong></p><p>​        1.解决了Sigmoid的输出不关于零点对称的问题</p><p>​        2.也具有Sigmoid的平滑、容易求导的优点</p><p>​        <strong>缺点：</strong></p><p>​        1.运算量过大</p><p>​        2.梯度消失没有解决</p><p>​        <strong>函数及其导数的代码参考实现如下：</strong></p><p>​                    </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""tanh函数"""</span><br>    <span class="hljs-keyword">return</span> ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dx_tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""tanh函数的导数"""</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - tanh(x) * tanh(x)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = tanh(x)<br>    dx_fx = dx_tanh(x)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'tanh 函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'tanh函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""tanh函数"""</span><br>    <span class="hljs-keyword">return</span> ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dx_tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""tanh函数的导数"""</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - tanh(x) * tanh(x)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = tanh(x)<br>    dx_fx = dx_tanh(x)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'tanh 函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'tanh函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'dx_fx'</span>)<br>    plt.plot(x, dx_fx)<br>    plt.show()<br></code></pre></td></tr></tbody></table></figure><ul><li><h2 id="Relu激活函数"><a href="#Relu激活函数" class="headerlink" title="Relu激活函数"></a><strong>Relu激活函数</strong></h2><p>受到了step函数的生物学启发，当输入为正的时候，导数不为零，允许基于梯度的学习。在输入为负值的时候，学习速度会变得很慢。函数的定义为：</p><script type="math/tex; mode=display">f(x) = max(0,x)</script><p>函数图像如下：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img6.png" alt="img6"></p><p>函数的导数如下：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img7.png" alt="img7"></p><p><strong>优点：</strong>1.相比于Sigmoid和tanh函数，ReLU在SGD中能够快速收敛，它具有线性、非饱和的形式</p><p>​            2.有效缓解了梯度消失的问题</p><p>​            3.SIgmoid和tanh设计了很多expensive的操作，ReLU可以更加简单的实现。</p><p>​            4.无监督预训练能有较好的表现</p><p><strong>缺点：</strong></p><p>​            1.输出不是zero-centered</p><p>​            2.死亡ReLU（Dead ReLU Problem），某些神经元可能永远不会被激活，导致参数不会被更新。</p><p>​            尽管存在这两个问题，ReLU仍然是最常用的激活函数，在搭建人工神经网络的时候推荐大家<strong>优先尝试！</strong></p><p><strong>函数及导数代码如下：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""relu函数"""</span><br>    <span class="hljs-comment"># temp = np.zeros_like(x)</span><br>    <span class="hljs-comment"># if_bigger_zero = (x &gt; temp)</span><br>    <span class="hljs-comment"># return x * if_bigger_zero</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, x)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dx_relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""relu函数的导数"""</span><br>    <span class="hljs-comment"># temp = np.zeros_like(x)</span><br>    <span class="hljs-comment"># if_bigger_equal_zero = (x &gt;= temp)</span><br>    <span class="hljs-comment"># return if_bigger_equal_zero * np.ones_like(x)</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment"># ---------------------------------------------</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = relu(x)<br>    dx_fx = dx_relu(x)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Relu函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Relu函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""relu函数"""</span><br>    <span class="hljs-comment"># temp = np.zeros_like(x)</span><br>    <span class="hljs-comment"># if_bigger_zero = (x &gt; temp)</span><br>    <span class="hljs-comment"># return x * if_bigger_zero</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, x)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dx_relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""relu函数的导数"""</span><br>    <span class="hljs-comment"># temp = np.zeros_like(x)</span><br>    <span class="hljs-comment"># if_bigger_equal_zero = (x &gt;= temp)</span><br>    <span class="hljs-comment"># return if_bigger_equal_zero * np.ones_like(x)</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment"># ---------------------------------------------</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = relu(x)<br>    dx_fx = dx_relu(x)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Relu函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Relu函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'dx_fx'</span>)<br>    plt.plot(x, dx_fx)<br>    plt.show()<br><br></code></pre></td></tr></tbody></table></figure></li></ul><ul><li><h2 id="Leaky-ReLU函数（PReLU）"><a href="#Leaky-ReLU函数（PReLU）" class="headerlink" title="Leaky ReLU函数（PReLU）"></a><strong>Leaky ReLU函数（PReLU）</strong></h2><p>函数的定义为</p><script type="math/tex; mode=display">f(x) = max(ax,x)</script><p>函数的图像如下：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img8.png" alt="img8"></p></li></ul><p>​        函数的导数如下：</p><p>​        <img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img9.png" alt="img9"></p><p>​        <strong>特点：</strong>与ReLU相比，Leak给所有负值赋予了一个非零斜率，leak是一个很小的常数，保留了一些负轴的值，使得负轴的信息不会全部丢失。</p><p>​        <strong>函数及其导数代码如下：</strong></p><p>​        </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>]= [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">leaky_relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""leaky relu函数"""</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span> * x, x)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dx_leaky_relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""leaky relu函数的导数"""</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment"># ---------------------------------------------</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = leaky_relu(x)<br>    dx_fx = dx_leaky_relu(x)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Leaky ReLu函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Leaky Relu函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>]= [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">leaky_relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""leaky relu函数"""</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span> * x, x)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dx_leaky_relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""leaky relu函数的导数"""</span><br>    <span class="hljs-keyword">return</span> np.where(x &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment"># ---------------------------------------------</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = leaky_relu(x)<br>    dx_fx = dx_leaky_relu(x)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Leaky ReLu函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Leaky Relu函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'dx_fx'</span>)<br>    plt.plot(x, dx_fx)<br>    plt.show()<br><br></code></pre></td></tr></tbody></table></figure><ul><li><h2 id="Mish激活函数"><a href="#Mish激活函数" class="headerlink" title="Mish激活函数"></a><strong>Mish激活函数</strong></h2><p>函数定义如下</p><script type="math/tex; mode=display">f(x) = x*tanh(ln(1+e^{x}))</script><p>函数图像如下：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img10.png" alt="img10"></p><p>函数的导数如下：</p><p><img src="/2021/09/06/%E5%90%84%E7%B1%BB%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/img11.png" alt="img11"></p><p><strong>函数特点：</strong></p><p>1.无上界、有下界、平滑且非单调</p><p><strong>优点：</strong></p><p>1.防止了梯度消失</p><p>2.提升了网络的正则化效果</p><p>3.减少了一些不可预料的问题，使得网络更容易优化并且提高泛化性能。</p><p><strong>缺点：</strong></p><p>1.引入了指数函数，增加了计算量</p><p><strong>函数及其导数代码：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sech</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""sech函数"""</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> / (np.exp(x) + np.exp(-x))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""sigmoid函数"""</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softplus</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""softplus函数"""</span><br>    <span class="hljs-keyword">return</span> np.log10(<span class="hljs-number">1</span> + np.exp(x))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""tanh函数"""</span><br>    <span class="hljs-keyword">return</span> ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = x * tanh(softplus(x))<br>    dx_fx = sech(softplus(x)) * sech(softplus(x)) * x * sigmoid(x) + fx / x<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Mish函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Mish函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 解决中文显示问题</span><br>plt.rcParams[<span class="hljs-string">'font.sans-serif'</span>] = [<span class="hljs-string">'SimHei'</span>]<br>plt.rcParams[<span class="hljs-string">'axes.unicode_minus'</span>] = <span class="hljs-literal">False</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sech</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""sech函数"""</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> / (np.exp(x) + np.exp(-x))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""sigmoid函数"""</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softplus</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""softplus函数"""</span><br>    <span class="hljs-keyword">return</span> np.log10(<span class="hljs-number">1</span> + np.exp(x))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">"""tanh函数"""</span><br>    <span class="hljs-keyword">return</span> ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    x = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fx = x * tanh(softplus(x))<br>    dx_fx = sech(softplus(x)) * sech(softplus(x)) * x * sigmoid(x) + fx / x<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Mish函数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'fx'</span>)<br>    plt.plot(x, fx)<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    ax = plt.gca()  <span class="hljs-comment"># 得到图像的Axes对象</span><br>    ax.spines[<span class="hljs-string">'right'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像右边的轴设为透明</span><br>    ax.spines[<span class="hljs-string">'top'</span>].set_color(<span class="hljs-string">'none'</span>)  <span class="hljs-comment"># 将图像上面的轴设为透明</span><br>    ax.xaxis.set_ticks_position(<span class="hljs-string">'bottom'</span>)  <span class="hljs-comment"># 将x轴刻度设在下面的坐标轴上</span><br>    ax.yaxis.set_ticks_position(<span class="hljs-string">'left'</span>)  <span class="hljs-comment"># 将y轴刻度设在左边的坐标轴上</span><br>    ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 将两个坐标轴的位置设在数据点原点</span><br>    ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>, <span class="hljs-number">0</span>))<br>    plt.title(<span class="hljs-string">'Mish函数的导数'</span>)<br>    plt.xlabel(<span class="hljs-string">'x'</span>)<br>    plt.ylabel(<span class="hljs-string">'dx_fx'</span>)<br>    plt.plot(x, dx_fx)<br>    plt.show()<br><br></code></pre></td></tr></tbody></table></figure></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​        在深度学习的发展中，出现了许多的激活函数，那么我们改如何选择激活函数呢？</p><p>​        1.由于需要花费大量时间来处理大量数据，收敛的速度尤其重要。所以尽量要使用zero-centered数据和zero-centered输出，加快收敛速度。</p><p>​        2.如果使用了ReLU，一定要注意学习率的设置，或者你可以尝试Leaky ReLU或者PReLU等等</p><p>​        3.sigmoid做为远古的激活函数，已经很少被使用！</p><p>​            （忘了softmax，如果我还记得的话会补上</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MGN_net</title>
    <link href="/2021/09/06/MGN-net/"/>
    <url>/2021/09/06/MGN-net/</url>
    
    <content type="html"><![CDATA[<h1 id="Learning-Discriminative-Features-with-Multiple-Granularities-for-Person-Re-Identification"><a href="#Learning-Discriminative-Features-with-Multiple-Granularities-for-Person-Re-Identification" class="headerlink" title="Learning Discriminative Features with Multiple Granularities for Person Re-Identification"></a>Learning Discriminative Features with Multiple Granularities for Person Re-Identification</h1><p>这是一篇关于行人重识别的笔记，学习了云从科技的MGN网络。原论文：<a href="https://arxiv.org/pdf/1804.01438.pdf">Learning Discriminative Features with Multiple Granularities for Person Re-Identification</a></p><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a><strong>摘要：</strong></h2><p>The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification(Re-ID)tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities We carefully design the Multiple Granularity Network(MGN),a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feaure representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, Duke MTMC-reid and CUHKO3 indicate that our method robustly achieves state-of-the -art performances and outperforms any existing approaches by a large margin For example, on Market-1501 dataset in single query mode, we obtain a top result of Rank-1/mAP=96.6%/94.2% with this method after re-ranking。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本文提出了一种网络结构，将全局特征跟多粒度局部特征结合在一起，全局特征负责整体的宏观上大家共有的特征的提取，然后把图像切分成不同块，每一块不同粒度，负责不同层次或者不同级别特征的提取。在观察中发现，确实随着分割力度的增加，模型能够学到更详细的细节，最终产生MGN的网络结构。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul><li>表征学习：通过设计分类损失与对比损失，实现对网络的监督学习。</li><li>度量学习：基于TripletLoss三元损失的ReID方案。</li><li>局部特征学习：基于局部区域调整的ReID解决方案。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul><li><p><strong>多粒度特征</strong>：上图第一列有3张图，中间一列把3张图二分之一上下均分，第三列是把人从上到下分成三块——头部、腹部、腿部，它有3个粒度，每个粒度做独立的引导，使得模型尽量对每个粒度学习更多信息。</p><p>下图表示的是注意力的呈现效果，这不是基于本模型产生的，是基于之前的算法看到的。上面的图是整张图在输入时网络在关注什么，整个人看着比较均匀，范围更广。第三栏从上到下相当于切成3块，每一块看的时候它的关注点会更加集中一点，亮度分布不会像上图那么均匀，更关注局部的亮点，可以理解为网络在关注不同粒度的信息。</p><p><img src="/2021/09/06/MGN-net/img1.png" alt="img1"></p><p><img src="/2021/09/06/MGN-net/img2.png" alt="img2"></p></li></ul><ul><li><p><strong>网络结构：</strong></p><p>如下图，输入的尺寸为 384×128，用的backbone为ResNet-50，如果在不做任何改变的情况下，它的特征图谱输出尺寸，从下方表格可以看到，global 这个地方就相当于对 Resnet 50不做任何的改变，特征图谱输出是 12×4。</p><p>下面有一个 part-2 跟 part-3，这是在 Res4_1 的位置，本来是有一个stride 等于 2 的下采样的操作，我们把 2 改成 1，没有下采样，这个地方的尺寸就不会缩小 2，所以 part-2 跟 part-3 比 global 大一倍的尺寸，它的尺寸是 24×8。为什么要这么操作？因为会强制分配 part-2 跟 part-3 去学习细粒度特征，如果把特征尺寸做得大一点，相当于信息更多一点，更利于网络学到更细节的特征。</p><p>网络结构从左到右，先是两个人的图片输入，这边有 3 个模块。3 个模块的意思是表示 3 个分支共享网络，前三层这三个分支是共享的，到第四层时分成三个支路，第一个支路是 global 的分支，第二个是 part-2 的分支，第三个是 part-3 的分支。在 global 的地方有两块，右边这个方块比左边的方块大概缩小了一倍，因为做了个下采样，下面两个分支没有做下采样，所以第四层和第五层特征图是一样大小的。</p><p>接下来我们对 part-2 跟 part-3 做一个从上到下的纵向分割，part-2 在第五层特征图谱分成两块，part-3 对特征图谱从上到下分成三块。在分割完成后，我们做一个 pooling，相当于求一个最值，我们用的是 Max-pooling，得到一个 2048 的向量，这个是长条形的、横向的、黄色区域这个地方。</p><p>但是 part-2 跟 part-3 的操作跟 global 是不一样的，part-2 有两个 pooling，第一个是蓝色的，两个 part 合在一起做一个 global-pooling，我们强制 part-2 去学习细节的联合信息，part-2 有两个细的长条形，就是刚才引导它去学细节型的信息。淡蓝色这个地方变成小方体一样，是做降维，从 2048 维做成 256 维，这个主要方便特征计算，因为可以降维，更快更有效。在测试的时候会在淡蓝色的地方，小方块从上到下应该是 8 个，我们把这 8 个 256 维的特征串连一个 2048 的特征，用这个特征替代前面输入的图片。</p></li></ul><p>  <img src="/2021/09/06/MGN-net/img4.png" alt="img4"></p><p>​                     <img src="/2021/09/06/MGN-net/img3.png" alt="img3"></p><ul><li><p><strong>Loss设计：</strong>Loss说简单也简单，说复杂也复杂。从头到尾就用了两种Loss，一个是<a href="https://blog.csdn.net/luoxuexiong/article/details/90062937?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163084429216780264058404%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163084429216780264058404&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v29_ecpm-5-90062937.first_rank_v2_pc_rank_v29&amp;utm_term=softmaxloss&amp;spm=1018.2226.3001.4187">SoftmaxLoss</a>，一个是<a href="https://blog.csdn.net/jcjx0315/article/details/77160273?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163084436916780366577047%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163084436916780366577047&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-77160273.first_rank_v2_pc_rank_v29&amp;utm_term=tripletloss&amp;spm=1018.2226.3001.4187">TripletLoss</a>(这两个Loss有空写一下现在就将就放个链接)。</p><p>分别如下两图所示</p><script type="math/tex; mode=display">L_{softmax} = -\sum_{i=1}^{N}log\frac{e^{w^{T}_{yi}}f_{i}}{\sum_{k=1}^{C}e^{w^{T}_{k}}f_{i}}\tag 1</script><script type="math/tex; mode=display">L_{triplet}=-\sum_{i=1}^{p}\sum_{a=1}^{k}[a+\underset{p=1...k}{max}||f_{a}^{(i)}-f_{p}^{(i)}||_{2}-\min_{n=1...K\\j=1...P }||f_{a}^{(i)}-f_{=n}^{(i)}||_{2}]\tag 2</script><p>在global分支上，对2048维做了SoftmaxLoss，对256维做了TripletLoss，这是对global信息通用的方法。下面两个部分global的处理方式也是一样的。中间part-2有一个全局信息，做两个Loss。</p><p>但是，下面两个 Local 特征看不到 TripletLoss，只用了 SoftmaxLoss，如果对细节当和分支做 TripletLoss，效果会变差。为什么效果会变差？</p><p>一张图片分成从上到下两部分的时候，最完美的情况当然是上面部分是上半身，下面部分是下半身，但是在实际的图片中，有可能整个人都在上半部分，下半部分全是背景，这种情况用上、下部分来区分，假设下半部分都是背景，把这个背景放到 TripletLoss 三元损失里去算这个 Loss，就会使得这个模型学到莫名其妙的特征。</p><p>比如背景图是个树，另外一张图是某个人的下半身，比如一个女生的下半身是一个裙子，你让裙子跟另外图的树去算距离，无论是同类还是不同类，算出来的距离是没有任何物理意义或实际意义的。从模型的角度来讲，它属于污点数据，这个污点数据会引导整个模型崩溃掉或者学到错误信息，使得预测的时候引起错误。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​        在使用SGD momentum=0.9，Learning rate=0.01的情况下，做了weight decay = 0.0005，分别在第40个epoch和第60个epoch，在Market1501训练80个epochs，Rank1达到了95.7%，mAP为86.9%。</p><p>​        对一些已知测试集数据分布的情况下，可以用<a href="https://blog.csdn.net/lwplwf/article/details/84862054?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163084500416780255262323%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163084500416780255262323&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~sobaiduend~default-1-84862054.pc_v2_rank_blog_default&amp;utm_term=rerank&amp;spm=1018.2226.3001.4450">ReRank</a>技术把指标大大提高。</p><p>​        基于paddlepaddle深度学习框架的代码已复现，并开源至<a href="https://github.com/Mr-ind1fferent/Re-ID_MGN_Paddlepaddle">https://github.com/Mr-ind1fferent/Re-ID_MGN_Paddlepaddle</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
